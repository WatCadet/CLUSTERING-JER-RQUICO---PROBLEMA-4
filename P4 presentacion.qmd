---
title: "P4 -  Segmentación de clientes utilizando métodos de clustering jerárquicos"
subtitle: "20582 - Análisis de Datos para el GMAT"
format:
  html:
    theme: lumen
    toc: true
    toc-depth: 3
Rendering:
    embed-resources: true
author: 
  - Dídac Capó, Irene Rodríguez y Carlos Aliño
---

<style>
p {
  text-align: justify;
}
</style>


[Enlace al repositorio de Github](https://github.com/WatCadet/CLUSTERING_JERARQUICO_P4)

```{r, echo=FALSE, eval=TRUE, message=FALSE}
library(tidyverse)
library(dplyr)
library(factoextra)
library(GGally)
library(ggplot2)
library(cluster)
library(caret)      
```
# Problema 4

Una empresa de comercio busca identificar segmentos de clientes según sus patrones de compra. Para ello, se dispone de un conjunto de datos ficticio denominado [data_comercio.csv](https://github.com/igmuib/Practica_AD/blob/main/data_comercio.csv). Este conjunto de datos incluye información sobre la identificación del cliente, la categoría de las compras realizadas, el importe gastado en euros, el número de artículos adquiridos y la fecha de la última compra. La tarea consiste en realizar una segmentación de clientes utilizando métodos de clustering jerárquicos con diferentes enlaces y distancias.

Deberéis presentar visualizaciones de los agrupamientos obtenidos con cada método y comparar las diferencias entre ellos, destacando las ventajas y desventajas de cada enfoque. Además, se espera un análisis de las características principales de cada clúster, identificando, por ejemplo, a los clientes con mayor gasto promedio, el número promedio de artículos comprados por grupo y otras características relevantes. Finalmente, con base en los segmentos identificados, se deben proponer recomendaciones estratégicas para diseñar campañas de marketing dirigidas específicamente a cada segmento.

## Lectura y limpieza de los datos

```{r}
# cargamos los datos
data <-read.csv("data_comercio.csv",
                   header=TRUE, sep=",")
# eliminamos el identificador
raw_data <- data %>% select(-1)
# filtramos las variables numericas
raw_data_numeric <- raw_data[, sapply(raw_data, is.numeric)]
```

## Análisis descriptivo

Analicemos nuestro conjunto de datos, que consta de 200 observaciones y 5 variables:

### Descripción variables
  - `cliente_id`: Es una variable utilizada como identificador único para cada muestra. Los nombres se construyen siguiendo un formato específico: la letra "C" seguida por un número que corresponde al número de la muestra.  
  - `categoria`: Es una variable cualitativa que clasifica a los individuos según el tipo de compra que han realizado. En la siguiente tabla se presentan las categorías disponibles junto con la cantidad de observaciones correspondientes a cada una:
  
```{r}
table(raw_data$categoria)
```
  
  - `importe_gastado`: Es una variable cuantitativa continua que representa el importe gastado, expresado en euros.
  - `num_articulos`: Es una variable cuantitativa discreta que representa el número de articulos comprados.
  - `ultima_compra`: Es una variable de tipo fecha. Nos indica la fecha de la ultima compra. 
  
Representamos con `ggpairs` la relación entre las tres variables `importe_gastado`, `num_articulos` y `categoria`.

```{r, message=FALSE}
d <- raw_data %>% select(categoria, importe_gastado, num_articulos)
ggpairs(d, mapping = aes(color= categoria,alpha=0.5), legend = 1) +
  theme_bw()
```

El gráfico no nos da mucha información porque no hay diferencias destacables a simple vista entre las 5 categorías. En particular, las cajas de los boxplot se solapan.

Visualizemos con más detalle el gráfico de puntos de las variables `importe_gastado` y `num_articulos` según la categoria: 

```{r}
d <- raw_data %>% select(categoria, importe_gastado, num_articulos)
ggplot(data = d, aes(x = importe_gastado, y = num_articulos, color = categoria)) +
  geom_point(size = 2.5) +
  theme_bw()
```
Los puntos de colores están mezclados. No es evidente una partición de los puntos por estas 5 categorías.

Observamos ahora las medianas de los datos por cada categoría:

```{r}
d %>%
  group_by(categoria) %>%
  summarise(
    mediana_importe = median(importe_gastado, na.rm = TRUE),
    mediana_articulos = median(num_articulos, na.rm = TRUE)
  )
```
Según el número de artículos se evindencian 2 clusters, uno con mediana $5$ formado por las categorías *alimentos, hogar y ropa* y el otro con mediana igual a $7$ formado por *electrónica y juguetes*.

Según el importe de gasto, podemos separar en 3 clusters: el de importe más alto el *hogar*, el siguiente más alto *electrónica y juguetes* y el de menor importe de gasto *alimentación y ropa*.

Por el método del codo, decidimos el número de clusters según donde está el pliegue principal.

```{r}
fviz_nbclust(x = raw_data_numeric, FUNcluster = kmeans, method = "wss",
             diss = dist(raw_data_numeric, method = "manhattan")) +
  geom_vline(xintercept = 3, linetype = 2)
```
Por lo tanto, tomamos $k=3$ y tenemos tres clusters. Entonces veremos si podemos interpretar los clusters según una clasificación con sentido.

## Aplicación y comparación de distintos métodos de clustering jerárquicos aglomerativos

Como la magnitud de los valores difiere notablemente entre variables, las escalamos.

```{r}
raw_data_numeric <- scale(raw_data_numeric, center = TRUE, scale = TRUE)
```

Apliquemos métodos de clustering jerárquico aglomerativo. Para ello debemos escoger una medida de distancia y un tipo de enlace. 

Como tenemos una variable cuantitativa discreta y otra variable cuantitativa continua la distancia más adecuada será la manhattan ya que es mejor en este caso que la euclida, puesto que esta última esta pensada para variables cuantitativas continuas.

Además, hemos considerado distintos enlaces (simple, completo, medio y de Ward) y hemos evaluado hasta qué punto su estructura refleja las distancias originales entre observaciones con el coeficiente de correlación entre las distancias cophenetic del dendrograma (altura de los nodos) y la matriz de distancias original. Hemos concluido que el enlace más adecuado es el medio.

```{r}
# Matriz de distancias
matriz_distancias2 <- dist(raw_data_numeric, method = "manhattan")

# Fijamos semilla
set.seed(123)

hc_manhattan_medio  <- hclust(d = matriz_distancias2, method = "average")

cor(x = matriz_distancias2, cophenetic(hc_manhattan_medio))
```

Representemos los dendogramas:

```{r}
fviz_dend(x = hc_manhattan_medio, k=3, cex = 0.6, main = "Dendrograma - Enlace Medio")
```

Veamos cómo ha quedado la clasificación.

```{r}
prop.table(table(cutree(hc_manhattan_medio, k = 3), raw_data$categoria))

addmargins(table(cutree(hc_manhattan_medio, k = 3), raw_data$categoria))

round(prop.table(table(cutree(hc_manhattan_medio, k = 3), raw_data$categoria), margin=1), 2)

round(prop.table(table(cutree(hc_manhattan_medio, k = 3), raw_data$categoria), margin=2), 2)
```
Recordemos que tenemos un total de:

```{r}
table(raw_data$categoria)
```

Interpretamos la clasificación. El máximo de elementos de cada categoría quedan clasificados en el cluster 1, que está muy equilibrado por categorías. El tercer cluster contiene principalmente electrónica y juguetes. El segundo cluster contiene principalmente Hogar. 

Observamos ahora las medianas de los datos por cada categoría:

```{r}
d %>%
  group_by(cutree(hc_manhattan_medio, k = 3))%>%
  summarise(
    mediana_importe = median(importe_gastado, na.rm = TRUE),
    mediana_articulos = median(num_articulos, na.rm = TRUE)
  )
```

## K-medoide

 Usamos la misma distancia que antes: Manhattan. Representamos el clustering k-medoides con Manhattan (sin jerarquía).

```{r}
# Aplicar PAM (Partitioning Around Medoids)
pam_cluster <- pam(raw_data_numeric, k = 3, metric="manhattan")

# Visualización de los clusters
fviz_cluster(pam_cluster, data = raw_data_numeric, ellipse.type = "t", palette = "jco", repel = TRUE) + 
  theme_bw() + 
  labs(title = "Clustering K-Medoides con Distancia Manhattan")
```

Veamos cómo ha quedado la clasificación.

```{r}
tabla2 = table(pam_cluster$clustering, raw_data$categoria)
tabla2

addmargins(tabla2)

round(prop.table(tabla2, margin=2), 2)
```

Recordemos que tenemos un total de:

```{r}
table(raw_data$categoria)
```

Alimentos, ropa y hogar tienen su máximo en el cluster 2. El cluster 1 está mayoritariamente representado por juguetes y en menor cantidad electrónica, y electrónica tiene su máximo en el cluster 3. 

```{r}
d %>%
  group_by(pam_cluster$clustering) %>%
  summarise(
    mediana_importe = median(importe_gastado, na.rm = TRUE),
    mediana_articulos = median(num_articulos, na.rm = TRUE)
  )
```

## Train - Test

Como hemos visto en las secciones anteriores, el algoritmo k-medoides ha demostrado ofrecer mejores resultados. Para validar este comportamiento, realizaremos una simulación dividiendo los datos en dos partes: utilizaremos el 80% de los datos para entrenar el modelo de k-medoides y el 20% restante se empleará como conjunto de prueba.

En esta etapa de evaluación, compararemos las predicciones generadas por el modelo con la clasificación original de los datos, lo que nos permitirá medir su precisión y rendimiento.

Separamos el conjunto de datos:

```{r}
set.seed(124) 
data <- raw_data_numeric

train_index <- createDataPartition(1:nrow(data), p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
test_labels <- pam_cluster$clustering[-train_index]
```
Entrenamos el modelo:
```{r}
k <- 3
kmedoids_model <- pam(train_data, k)
print(kmedoids_model)
```
Aplicamos el modelo al conjunto de prueba:
```{r}
medoids <- kmedoids_model$medoids
dist_test <- apply(test_data, 1, function(x) {
  apply(medoids, 1, function(y) sum(abs(x - y)))  # Distancia Manhattan
})

test_clusters <- apply(dist_test, 2, which.min)
```
Comparamos los resultados del modelo con los reales:
```{r}
cluster_factor <- factor(test_clusters, levels = 1:k)
sum(cluster_factor == test_labels)
```
Concluimos que para esta simulación el modelo ha acertado 35 veces de 40. 


## Recomendaciones

## Bibliografia

- [Ciencia de Datos](https://cienciadedatos.net/documentos/37_clustering_y_heatmaps#Hierarchical_clustering)
- [Aprender R - UIB](https://aprender-uib.github.io/AD/)
- [geeksforgeeks](https://www.geeksforgeeks.org/computing-classification-evaluation-metrics-in-r/)
 

