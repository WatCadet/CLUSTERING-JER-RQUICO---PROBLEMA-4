---
title: "P4 -  Segmentación de clientes utilizando métodos de clustering jerárquicos"
subtitle: "20582 - Análisis de Datos para el GMAT"
format:
  html:
    theme: lumen
    toc: true
    toc-depth: 3
Rendering:
    embed-resources: true
author: 
  - Dídac Capó, Irene Rodríguez y Carlos Aliño
---

<style>
p {
  text-align: justify;
}
</style>


[Enlace al repositorio de Github](https://github.com/WatCadet/CLUSTERING_JERARQUICO_P4)

```{r, echo=FALSE, eval=TRUE, message=FALSE}
library(tidyverse)
library(dplyr)
library(factoextra)
library(GGally)
library(ggplot2)
library(cluster)
```
# Problema 4

Una empresa de comercio busca identificar segmentos de clientes según sus patrones de compra. Para ello, se dispone de un conjunto de datos ficticio denominado [data_comercio.csv](https://github.com/igmuib/Practica_AD/blob/main/data_comercio.csv). Este conjunto de datos incluye información sobre la identificación del cliente, la categoría de las compras realizadas, el importe gastado en euros, el número de artículos adquiridos y la fecha de la última compra. La tarea consiste en realizar una segmentación de clientes utilizando métodos de clustering jerárquicos con diferentes enlaces y distancias.

Deberéis presentar visualizaciones de los agrupamientos obtenidos con cada método y comparar las diferencias entre ellos, destacando las ventajas y desventajas de cada enfoque. Además, se espera un análisis de las características principales de cada clúster, identificando, por ejemplo, a los clientes con mayor gasto promedio, el número promedio de artículos comprados por grupo y otras características relevantes. Finalmente, con base en los segmentos identificados, se deben proponer recomendaciones estratégicas para diseñar campañas de marketing dirigidas específicamente a cada segmento.


## Resumen 
## Lectura y limpieza de los datos

```{r}
# cargamos los datos
data <-read.csv("data_comercio.csv",
                   header=TRUE, sep=",")
# eliminamos el identificador
raw_data <- data %>% select(-1)
# filtramos las variables numericas
raw_data_numeric <- raw_data[, sapply(raw_data, is.numeric)]
```

## Análisis descriptivo

Analicemos nuestro conjunto de datos, que consta de 200 observaciones y 5 variables:

### Descripción variables
  - `cliente_id`: Es una variable utilizada como identificador único para cada muestra. Los nombres se construyen siguiendo un formato específico: la letra "C" seguida por un número que corresponde al número de la muestra.  
  - `categoria`: Es una variable cualitativa que clasifica a los individuos según el tipo de compra que han realizado. En la siguiente tabla se presentan las categorías disponibles junto con la cantidad de observaciones correspondientes a cada una:
  
```{r}
table(raw_data$categoria)
```
  
  - `importe_gastado`: Es una variable cuantitativa continua que representa el importe gastado, expresado en euros.
  - `num_articulos`: Es una variable cuantitativa discreta que representa el número de articulos comprados.
  - `ultima_compra`: Es una variable de tipo fecha. Nos indica la fecha de la ultima compra. 
  
Representamos con `ggpairs` la relación entre las tres variables `importe_gastado`, `num_articulos` y `categoria`.

```{r, message=FALSE}
d <- raw_data %>% select(categoria, importe_gastado, num_articulos)
ggpairs(d, mapping = aes(color= categoria), legend = 1) +
  theme_bw()
```

El gráfico no nos da mucha información porque no hay diferencias destacables a simple vista entre las 5 categorías. En particular, las cajas de los boxplot se solapan.

Visualizemos con más detalle el gráfico de puntos de las variables `importe_gastado` y `num_articulos` según la categoria: 

```{r}
d <- raw_data %>% select(categoria, importe_gastado, num_articulos)
ggplot(data = d, aes(x = importe_gastado, y = num_articulos, color = categoria)) +
  geom_point(size = 2.5) +
  theme_bw()
```
Los puntos de colores están mezclados. No es evidente una partición de los puntos por estas 5 categorías.

Observamos ahora las medianas de los datos por cada categoría:

```{r}
d %>%
  group_by(categoria) %>%
  summarise(
    mediana_importe = median(importe_gastado, na.rm = TRUE),
    mediana_articulos = median(num_articulos, na.rm = TRUE)
  )
```

Según el número de artículos se evindencian 2 clusters, uno con mediana $5$ formado por las categorías *alimentos, hogar y ropa* y el otro con mediana igual a $7$ formado por *electrónica y juguetes*.

Según el importe de gasto, podemos separar en 3 clusters: el de importe más alto el *hogar*, el siguiente más alto *alimentación y ropa* y el de menor importe de gasto *electrónica y juguetes*.

Por el método del codo, decidimos el número de clusters según dónde está el pliegue principal.

```{r}
fviz_nbclust(x = raw_data_numeric, FUNcluster = kmeans, method = "wss",
             diss = dist(raw_data_numeric, method = "manhattan")) +
  geom_vline(xintercept = 3, linetype = 2)
```
Por lo tanto, tomamos $k=3$ y tenemos tres clusters. Entonces veremos si podemos interpretar los clusters según la clasificación por importe de gastos.

## Aplicación y comparación de distintos métodos de clustering jerárquicos aglomerativos

Como la magnitud de los valores difiere notablemente entre variables, las escalamos.

```{r}
raw_data_numeric <- scale(raw_data_numeric, center = TRUE, scale = TRUE)
```

Apliquemos métodos de clustering jerárquico aglomerativo. Para ello debemos escoger una medida de distancia y un tipo de enlace. Veamos los casos estudiados:  

### Distancia Euclidea

En este caso, empleamos la función hclust(), a la que se pasa como argumento una matriz de distancia euclidea y el tipo de enlace Se comparan los resultados con los enlaces simple, completo, medio y de Ward:

```{r}
# Matriz de distancias
matriz_distancias <- dist(raw_data_numeric, method = "euclidean")

# Fijamos semilla
set.seed(123)

hc_euclidea_simple   <- hclust(d = matriz_distancias, method = "single")
hc_euclidea_completo <- hclust(d = matriz_distancias, method = "complete")
hc_euclidea_medio  <- hclust(d = matriz_distancias, method = "average")
hc_euclidea_ward  <- hclust(d = matriz_distancias, method = "ward.D2")
```

Representemos los dendogramas:

Pintando $k=5$:

```{r}
fviz_dend(x = hc_euclidea_simple, k = 5, cex = 0.6, main = "Dendrograma - Enlace Simple")
fviz_dend(x = hc_euclidea_completo, k = 5, cex = 0.6, main = "Dendrograma - Enlace Completo")
  # + theme(plot.title =  element_text(hjust = 0.5, size = 15))
fviz_dend(x = hc_euclidea_medio, k = 5, cex = 0.6, main = "Dendrograma - Enlace Medio")
fviz_dend(x = hc_euclidea_ward, k = 5, cex = 0.6, main = "Dendrograma - Enlace de Ward")
```

Sabemos que existen 5 grupos en la población, evaluemos ahora que enlace consigue los mejores resultados. En este caso, los cuatro tipos identifican claramente 5 clusters. Aunque esto no significa que en los 4 dendrogramas los clusters estén formados por exactamente las mismas observaciones.

Evaluemos hasta qué punto su estructura refleja las distancias originales entre observaciones con el coeficiente de correlación entre las distancias cophenetic del dendrograma (altura de los nodos) y la matriz de distancias original:
```{r}
cor(x = matriz_distancias, cophenetic(hc_euclidea_simple))
cor(x = matriz_distancias, cophenetic(hc_euclidea_completo))
cor(x = matriz_distancias, cophenetic(hc_euclidea_medio))
cor(x = matriz_distancias, cophenetic(hc_euclidea_ward))
```
Cuanto más cercano es el valor a 1, mejor refleja el dendrograma la verdadera similitud entre las observaciones. En este caso, el método de enlace medio consigue representar ligeramente mejor la similitud entre observaciones, seguido por el enlace completo y el de Ward. En este caso, el enlace simple es el que da peores resultados. 



Ahora tenemos que decidir a qué altura cortamos para generar los clusters. La función `cutree()` nos devuelve el cluster al que se ha asignado cada observación dependiendo del número de clusters especificado:

```{r}
cutree(hc_euclidea_medio, k = 5)
```

Una forma visual de comprobar los errores en las asignaciones es indicando en el argumento labels el grupo real al que pertenece cada observación. Si la agrupación resultante coincide con los grupos reales, entonces, dentro de cada clusters las labels serán las mismas.

```{r}
table(cutree(hc_euclidea_medio, k = 5), raw_data$categoria)
```

----------

Validación. D. Eculidea, Enlace Completo.

```{r}
fviz_dend(x = hc_euclidea_medio, k = 5, cex = 0.5, main = "Enlace completo", k_colors = "jco",
          sub = "Distancia euclídea") +
  theme(plot.title =  element_text(hjust = 0.5, size = 15))

hkmeans_cluster <- hkmeans(x = raw_data_numeric, hc.metric = "euclidean",
                           hc.method = "average", k = 5)

hkmeans_cluster

fviz_cluster(object = hkmeans_cluster, pallete = "jco", repel = TRUE) +
  theme_bw() + labs(title = "Hierarchical k-means Clustering")
```


Pintando $k=4$:

```{r}
fviz_dend(x = hc_euclidea_simple, k = 3, cex = 0.6, main = "Dendrograma - Enlace Simple")
fviz_dend(x = hc_euclidea_completo, k = 3, cex = 0.6, main = "Dendrograma - Enlace Completo")
  # + theme(plot.title =  element_text(hjust = 0.5, size = 15))
fviz_dend(x = hc_euclidea_medio, k = 3, cex = 0.6, main = "Dendrograma - Enlace Medio")
fviz_dend(x = hc_euclidea_ward, k = 3, cex = 0.6, main = "Dendrograma - Enlace de Ward")


table(cutree(hc_euclidea_completo, k = 3), raw_data$categoria)
```

### Distancia Manhatta

Hacemos lo mismo con la distància de Manhattan

```{r}
# Matriz de distancias
matriz_distancias2 <- dist(raw_data_numeric, method = "manhattan")

# Fijamos semilla
set.seed(123)

hc_manhattan_simple   <- hclust(d = matriz_distancias2, method = "single")
hc_manhattan_completo <- hclust(d = matriz_distancias2, method = "complete")
hc_manhattan_medio  <- hclust(d = matriz_distancias2, method = "average")
hc_manhattan_ward  <- hclust(d = matriz_distancias2, method = "ward.D2")
```

Representemos los dendogramas:

```{r}
fviz_dend(x = hc_manhattan_simple, cex = 0.6, main = "Dendrograma - Enlace Simple")
fviz_dend(x = hc_manhattan_completo, cex = 0.6, main = "Dendrograma - Enlace Completo")
fviz_dend(x = hc_manhattan_medio, k=3, cex = 0.6, main = "Dendrograma - Enlace Medio")
fviz_dend(x = hc_manhattan_ward, cex = 0.6, main = "Dendrograma - Enlace de Ward")
```

Evaluemos hasta qué punto su estructura refleja las distancias originales entre observaciones con el coeficiente de correlación entre las distancias cophenetic del dendrograma (altura de los nodos) y la matriz de distancias original:
```{r}
cor(x = matriz_distancias2, cophenetic(hc_manhattan_simple))
cor(x = matriz_distancias2, cophenetic(hc_manhattan_completo))
cor(x = matriz_distancias2, cophenetic(hc_manhattan_medio))
cor(x = matriz_distancias2, cophenetic(hc_manhattan_ward))
```
Ahora tenemos que decidir a qué altura cortamos para generar los clusters. La función `cutree()` nos devuelve el cluster al que se ha asignado cada observación dependiendo del número de clusters especificado:

```{r}
cutree(hc_manhattan_medio, k = 5)
```

Una forma visual de comprobar los errores en las asignaciones es indicando en el argumento labels el grupo real al que pertenece cada observación. Si la agrupación resultante coincide con los grupos reales, entonces, dentro de cada clusters las labels serán las mismas.

```{r}
table(cutree(hc_manhattan_medio, k = 3), raw_data$categoria)
table(cutree(hc_manhattan_medio, k = 4), raw_data$categoria)

```

## Recomendaciones

## Bibliografia

- [Ciencia de Datos](https://cienciadedatos.net/documentos/37_clustering_y_heatmaps#Hierarchical_clustering)
 - [Aprender R - UIB](https://aprender-uib.github.io/AD/)

