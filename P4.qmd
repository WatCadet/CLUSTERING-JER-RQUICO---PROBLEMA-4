---
title: "P4 -  Segmentación de clientes utilizando métodos de clustering jerárquicos"
subtitle: "20582 - Análisis de Datos para el GMAT"
format:
  html:
    theme: lumen
    toc: true
    toc-depth: 3
Rendering:
    embed-resources: true
author: 
  - Dídac Capó, Irene Rodríguez y Carlos Aliño
---

<style>
p {
  text-align: justify;
}
</style>


[Enlace al repositorio de Github](https://github.com/WatCadet/CLUSTERING_JERARQUICO_P4)

```{r setup, echo=FALSE, eval=TRUE, message=FALSE}
library(tidyverse)
library(dplyr)
library(factoextra)
library(GGally)
library(ggplot2)
library(cluster)
library(caret)      
```

# Problema 4

Una empresa de comercio busca identificar segmentos de clientes según sus patrones de compra. Para ello, se dispone de un conjunto de datos ficticio denominado [data_comercio.csv](https://github.com/igmuib/Practica_AD/blob/main/data_comercio.csv). Este conjunto de datos incluye información sobre la identificación del cliente, la categoría de las compras realizadas, el importe gastado en euros, el número de artículos adquiridos y la fecha de la última compra. La tarea consiste en realizar una segmentación de clientes utilizando métodos de clustering jerárquicos con diferentes enlaces y distancias.

Deberéis presentar visualizaciones de los agrupamientos obtenidos con cada método y comparar las diferencias entre ellos, destacando las ventajas y desventajas de cada enfoque. Además, se espera un análisis de las características principales de cada clúster, identificando, por ejemplo, a los clientes con mayor gasto promedio, el número promedio de artículos comprados por grupo y otras características relevantes. Finalmente, con base en los segmentos identificados, se deben proponer recomendaciones estratégicas para diseñar campañas de marketing dirigidas específicamente a cada segmento.

## Lectura y limpieza de los datos

Cargamos y visualizamos el conjunto de datos. Eliminamos el identificador y filtramos las variables numéricas. 

```{r, echo=FALSE}
# cargamos los datos
data <-read.csv("data_comercio.csv",
                   header=TRUE, sep=",")
# eliminamos el identificador
raw_data <- data %>% select(-1, -2)

# categoria a factor
raw_data$categoria <- as.factor(raw_data$categoria)

# fecha a Date
raw_data$ultima_compra <- as.Date(raw_data$ultima_compra)

# visualizamos
raw_data %>%
  glimpse

# filtramos las variables numericas
raw_data_numeric <- raw_data[, sapply(raw_data, is.numeric)]
```
## Análisis descriptivo

Analicemos nuestro conjunto de datos, que consta de 200 observaciones y 5 variables:

### Descripción variables

-   `cliente_id`: Es una variable utilizada como identificador único para cada muestra. Los nombres se construyen siguiendo un formato específico: la letra "C" seguida por un número que corresponde al número de la muestra.\
-   `categoria`: Es una variable cualitativa que clasifica a los individuos según el tipo de compra que han realizado. En la siguiente tabla se presentan las categorías disponibles junto con la cantidad de observaciones correspondientes a cada una:

```{r, echo=FALSE}
table(raw_data$categoria)
```

-   `importe_gastado`: Es una variable cuantitativa continua que representa el importe gastado, expresado en euros.
-   `num_articulos`: Es una variable cuantitativa discreta que representa el número de articulos comprados.
-   `ultima_compra`: Es una variable de tipo fecha. Nos indica la fecha de la ultima compra.

Representamos con `ggpairs` la relación entre las tres variables `importe_gastado`, `num_articulos` y `categoria`.

```{r, echo=FALSE, message=FALSE}
d <- raw_data %>% select(categoria, importe_gastado, num_articulos)
ggpairs(d, mapping = aes(color= categoria, alpha=0.6)) +
  theme_bw()
```

El gráfico no nos da mucha información porque no hay diferencias globales evidentes entre las 5 categorías. En particular, las cajas de los boxplot se solapan, aunque podemos ver las categorías distinguidas según su mediana del número de artículos.

Visualizemos con más detalle el gráfico de puntos de las variables `importe_gastado` y `num_articulos` según la categoria:

```{r, echo=FALSE}
d <- raw_data %>% select(categoria, importe_gastado, num_articulos)
ggplot(data = d, aes(x = importe_gastado, y = num_articulos, color = categoria)) +
  geom_point(size = 2.5) +
  theme_bw()
```

Los puntos de colores están mezclados. No es evidente una partición de los puntos por estas 5 categorías.

Observamos ahora las medianas de los datos por cada categoría:

```{r, echo=FALSE}
d %>%
  group_by(categoria) %>%
  summarise(
    mediana_importe = median(importe_gastado, na.rm = TRUE),
    mediana_articulos = median(num_articulos, na.rm = TRUE)
  )
```

Según el número de artículos se evindencian 2 clusters: uno con mediana $5$ formado por las categorías **alimentos, hogar y ropa** y el otro con mediana igual a $7$ formado por **electrónica y juguetes**.

Según el importe de gasto, podemos separar en 3 clusters: el de importe más alto el **hogar**, el siguiente más alto **electrónica y juguetes** y el de menor importe de gasto **alimentación y ropa**.

Por el método del **codo**, decidimos el número de clusters según dónde está el pliegue principal.

```{r, echo=FALSE}
fviz_nbclust(x = raw_data_numeric, FUNcluster = kmeans, method = "wss",
             diss = dist(raw_data_numeric, method = "manhattan")) +
  geom_vline(xintercept = 3, linetype = 2)
```

Por lo tanto, tomamos $k=3$ y tenemos tres clusters. Entonces veremos si podemos interpretar los clusters según una clasificación con sentido.

## Aplicación y comparación de distintos métodos de clustering jerárquicos aglomerativos

Como la magnitud de los valores difiere notablemente entre variables, las escalamos.

```{r}
raw_data_numeric <- scale(raw_data_numeric, center = TRUE, scale = TRUE)
```

Apliquemos métodos de clustering jerárquico aglomerativo. Para ello debemos escoger una medida de distancia y un tipo de enlace. Veamos los casos estudiados:  

### Distancia Euclidea

En este caso, empleamos la función `hclust()`, a la que se pasa como argumento una matriz de distancia euclidea y el tipo de enlace. Se comparan los resultados con los enlaces simple, completo, medio y de Ward:

```{r}
# Matriz de distancias
matriz_distancias <- dist(raw_data_numeric, method = "euclidean")

# Fijamos semilla
set.seed(123)

hc_euclidea_simple   <- hclust(d = matriz_distancias, method = "single")
hc_euclidea_completo <- hclust(d = matriz_distancias, method = "complete")
hc_euclidea_medio  <- hclust(d = matriz_distancias, method = "average")
hc_euclidea_ward  <- hclust(d = matriz_distancias, method = "ward.D2")
```

Representemos los dendogramas:

Pintando $k=3$:

```{r, warning = FALSE, echo=FALSE}
fviz_dend(x = hc_euclidea_simple, k = 3, cex = 0.6, main = "Dendrograma - Enlace Simple")
fviz_dend(x = hc_euclidea_completo, k = 3, cex = 0.6, main = "Dendrograma - Enlace Completo")
fviz_dend(x = hc_euclidea_medio, k = 3, cex = 0.6, main = "Dendrograma - Enlace Medio")
fviz_dend(x = hc_euclidea_ward, k = 3, cex = 0.6, main = "Dendrograma - Enlace de Ward")
```

El método del codo indica que consideremos que existen 3 grupos en la población. Evaluemos ahora que enlace consigue los mejores resultados. En este caso, los cuatro tipos identifican 3 clusters pero son claramente distintos en cuanto al número de obervaciones que contiene cada cluster. Además, sabemos que no tiene por qué ser cierto que en los 4 dendrogramas los clusters estén formados por exactamente las mismas observaciones.

Evaluemos hasta qué punto su estructura refleja las distancias originales entre observaciones con el coeficiente de correlación entre las distancias cophenetic del dendrograma (altura de los nodos) y la matriz de distancias original:

```{r}
cor(x = matriz_distancias, cophenetic(hc_euclidea_simple))
cor(x = matriz_distancias, cophenetic(hc_euclidea_completo))
cor(x = matriz_distancias, cophenetic(hc_euclidea_medio))
cor(x = matriz_distancias, cophenetic(hc_euclidea_ward))
```
Cuanto más cercano es el valor a 1, mejor refleja el dendrograma la verdadera similitud entre las observaciones. En este caso, el método de enlace medio consigue representar ligeramente mejor la similitud entre observaciones, seguido por el enlace completo y el de Ward. En este caso, el enlace simple es el que da peores resultados. 

Ahora tenemos que decidir a qué altura cortamos para generar los clusters. La función `cutree()` nos devuelve el cluster al que se ha asignado cada observación dependiendo del número de clusters especificado.

```{r, echo=FALSE}
cutree(hc_euclidea_medio, k = 3)
```

Una forma visual de comprobar los errores en las asignaciones es indicando en el argumento labels el grupo real al que pertenece cada observación. Si la agrupación resultante coincide con los grupos reales, entonces, dentro de cada clusters las labels serán las mismas.

```{r, echo=FALSE}
table(cutree(hc_euclidea_medio, k = 3), raw_data$categoria)
```

### Distancia Manhattan

Hacemos lo mismo con la distància de Manhattan.

```{r}
# Matriz de distancias
matriz_distancias2 <- dist(raw_data_numeric, method = "manhattan")

# Fijamos semilla
set.seed(123)

hc_manhattan_simple   <- hclust(d = matriz_distancias2, method = "single")
hc_manhattan_completo <- hclust(d = matriz_distancias2, method = "complete")
hc_manhattan_medio  <- hclust(d = matriz_distancias2, method = "average")
hc_manhattan_ward  <- hclust(d = matriz_distancias2, method = "ward.D2")
```

Representemos los dendogramas:

```{r, echo=FALSE}
fviz_dend(x = hc_manhattan_simple, k=3, cex = 0.6, main = "Dendrograma - Enlace Simple")
fviz_dend(x = hc_manhattan_completo, k=3, cex = 0.6, main = "Dendrograma - Enlace Completo")
fviz_dend(x = hc_manhattan_medio, k=3, cex = 0.6, main = "Dendrograma - Enlace Medio")
fviz_dend(x = hc_manhattan_ward, k=3, cex = 0.6, main = "Dendrograma - Enlace de Ward")
```

Evaluemos hasta qué punto su estructura refleja las distancias originales entre observaciones con el coeficiente de correlación entre las distancias cophenetic del dendrograma (altura de los nodos) y la matriz de distancias original:

```{r}
cor(x = matriz_distancias2, cophenetic(hc_manhattan_simple))
cor(x = matriz_distancias2, cophenetic(hc_manhattan_completo))
cor(x = matriz_distancias2, cophenetic(hc_manhattan_medio))
cor(x = matriz_distancias2, cophenetic(hc_manhattan_ward))
```

Ahora tenemos que decidir a qué altura cortamos para generar los clusters. La función `cutree()` nos devuelve el cluster al que se ha asignado cada observación dependiendo del número de clusters especificado.

```{r, echo=FALSE, include=FALSE}
cutree(hc_manhattan_medio, k = 3)
```

Una forma visual de comprobar los errores en las asignaciones es indicando en el argumento labels el grupo real al que pertenece cada observación. Si la agrupación resultante coincide con los grupos reales, entonces, dentro de cada clusters las labels serán las mismas.

```{r, echo=FALSE}
table(cutree(hc_manhattan_medio, k = 3), raw_data$categoria)
```
## Método de clustering jerárquicos aglomerativo escogido

### Escogemos distancia y tipo de enlace

Como la magnitud de los valores difiere notablemente entre variables, las hemos escalado. Apliquemos el método de clustering jerárquico aglomerativo. Para ello debemos escoger una medida de **distancia** y un tipo de **enlace**.

Como tenemos una variable cuantitativa **discreta** y otra variable cuantitativa continua la distancia más adecuada será la **Manhattan**. En este caso, consideramos que es mejor que la euclídea puesto que esta última está pensada para variables cuantitativas continuas.

Además, hemos considerado distintos enlaces (simple, completo, medio y de Ward) y hemos evaluado hasta qué punto su estructura refleja las distancias originales entre observaciones con el coeficiente de correlación entre las distancias cophenetic del dendrograma (altura de los nodos) y la matriz de distancias original. Hemos concluido que el enlace más adecuado es el **enlace medio**, con el coeficiente más alto.

### Interpretamos el resultado

Representemos el dendograma. Recordemos que por el método del codo hemos obtenido 3 clusters.

```{r, echo=FALSE, warning=FALSE}
fviz_dend(x = hc_manhattan_medio, k=3, cex = 0.6, main = "Dendrograma - Enlace Medio")
```

Notamos que el primer cluster tiene muchos más elementos que los otros dos. 

Veamos cómo ha quedado la clasificación.

```{r, echo=FALSE}
addmargins(table(cutree(hc_manhattan_medio, k = 3), raw_data$categoria))
```

Interpretamos la clasificación.  

Veamos, para cada categoría, cómo está repartida en los tres clusters. ¿En qué cluster se encuentra principalmente?

```{r, echo=FALSE}
round(prop.table(table(cutree(hc_manhattan_medio, k = 3), raw_data$categoria), margin=2), 2)
```
El máximo de elementos de cada categoría queda clasificado en el cluster 1, lo cual es coherente con el hecho de que el primer cluster tiene muchos más elementos que los otros dos. 

En segundo lugar, alimentos, hogar y ropa están tienen más observaciones en el cluster 2 que en el 3. 

Y electrónica y juguetes tienenmás observaciones en el cluster 3 que en el 2.

Veamos dentro de cada cluster qué categoría pesa más.

```{r, echo=FALSE}
round(prop.table(table(cutree(hc_manhattan_medio, k = 3), raw_data$categoria), margin=1), 2)
```

El cluster 1 está muy equilibrado por categorías. El cluster 2 contiene principalmente hogar. El cluster 3 contiene principalmente electrónica seguido de juguetes. 

Así, hemos obtenido una clasificación que interpretamos así:

  - Cluster sin categoría destacable donde residen la mayoría de observaciones.

  - Cluster principalmente de hogar y, con menor representación, alimentación y ropa.
  
  - Cluster principalmente de electrónica y juguetes.
  
Observamos ahora las medianas de los datos por cada cluster:

```{r, echo=FALSE}
d %>%
  group_by(cutree(hc_manhattan_medio, k = 3))%>%
  summarise(
    mediana_importe = median(importe_gastado, na.rm = TRUE),
    mediana_articulos = median(num_articulos, na.rm = TRUE)
  )
```

Analizamos las características principales de cada clúster:

  - Cluster 1: Mediana de importe de gasto $171.85$ con mediana de artículos $6$. Es el cluster con mediana de gasto más baja y mediana de número de artículos considerable, conteniendo así observaciones económicas.
  
  - Cluster 2: Mediana de importe de gasto $402.78$ con mediana de artículos $3$. Es el cluster con mediana de gasto más alta y menor mediana número de artículos. Por lo tanto, contiene artículos caros, lo cual es coherente con el hecho de que contiene principalmente artículos del hogar.

  - Cluster 3: Mediana de importe de gasto $383.765$ con mediana de artículos $8.5$. Es el cluster con mayor mediana de número de artículos y su mediana de gasto está entre las otras dos medianas, aunque más cercana a la alta. Recordemos que este cluster está principalmente representado por electrónica y juguetes.

## K-medoide

Como no hemos obtenido una clasificación satisfactoria por categorías --que tal vez no exista--, vamos a aplicar el métodos de clustering K-medoide (que en este caso tiene más sentido que el K-means porque tenemos una variable cuantitativa discreta). Compararemos la clasificación obtenida con la del jerárquico aglomerativo.

Usamos la misma distancia que antes: Manhattan. Representamos el clustering k-medoides con Manhattan.

```{r, echo=FALSE}
# Aplicar PAM (Partitioning Around Medoids)
pam_cluster <- pam(raw_data_numeric, k = 3, metric="manhattan")

# Visualización de los clusters
fviz_cluster(pam_cluster, data = raw_data_numeric, ellipse.type = "t", palette = "jco", repel = TRUE) + 
  theme_bw() + 
  labs(title = "Clustering K-Medoides con Distancia Manhattan")
```

Veamos cómo ha quedado la clasificación.

```{r, echo=FALSE}
tabla2 = table(pam_cluster$clustering, raw_data$categoria)

addmargins(tabla2)
```
Interpretamos la clasificación.  Esta vez la cantidad de elementos en cada cluster está más equilibrada.

Veamos, para cada categoría, cómo está repartida en los tres clusters. ¿En qué cluster tienen más observaciones?

```{r, echo=FALSE}
round(prop.table(tabla2, margin=2), 2)
```

Alimentos, hogar y ropa tienen su máximo en el cluster 2. Juguetes tiene su máximo en el cluster 1 y electrónica tiene su máximo en el cluster 3. 

Veamos dentro de cada cluster qué categoría pesa más.

```{r, echo=FALSE}
round(prop.table(tabla2, margin=1), 2)
```

El cluster 1 está mayoritariamente representado por juguetes y el cluster 3 tiene sobretot electrónica. 

Observamos ahora las medianas de los datos por cada cluster:

```{r, echo=FALSE}
d %>%
  group_by(pam_cluster$clustering) %>%
  summarise(
    mediana_importe = median(importe_gastado, na.rm = TRUE),
    mediana_articulos = median(num_articulos, na.rm = TRUE)
  )
```

Analizamos las características principales de cada clúster:

  - Cluster 1: Mediana de importe de gasto $147.76$ con mediana de artículos $8$. Es el cluster con mediana de gasto más baja, conteniendo así observaciones económicas. Recordemos que este cluster está principalmente representado por juguetes.
  
  - Cluster 2: Mediana de importe de gasto $232.97$ con mediana de artículos $3$. Este cluster tiene la menor mediana de número de artículos. Recordemos que este cluster lo asociamos principalmente a alimentos, ropa y hogar.

  - Cluster 3: Mediana de importe de gasto $403.84$ con mediana de artículos $8$. Es el cluster con mayor mediana de número de artículos y mayor mediana de gasto. Recordemos que este cluster está principalmente representado por electrónica.

## Decidimos entre jerárquico y K-medoide

En resumen, se pretendre ofrecer una segmentación de los clientes según su comportamiento de compra. Nos interesa la variable *categoría*.

Por una parte, del jerárquico hemos obtenido una clasificación que interpretamos así:

  - Cluster sin categoría destacable donde residen la mayoría de observaciones.

  - Cluster principalmente de hogar y, con menor representación, alimentación y ropa.
  
  - Cluster principalmente de electrónica y juguetes.

Por otra parte, del K-medoides hemos obtenido una clasificación que interpretamos así: 

  - Cluster de juguetes.

  - Cluster de alimentos, hogar y ropa.

  - Cluster de electrónica.
  
Recordemos que teníamos dos propuestas de partición y que con el método del codo hemos decidido que haya 3 clusters. Las dos propuestas eran:

1. Por importe de gasto:

  - Hogar.
  
  - Electrónica y juguetes.
  
  - Alimentos y ropa.
  
2. Por número de artículos:

  - Alimentos, hogar y ropa.
  
  - Electrónica y juguetes.

El problema del jerárquico es que el cluster en que no destaca ninguna categoría en concreto no interesa para segmentar a los clientes.

Los clusters del K-medoides, en cambio, sí que son adecuados para la interpretación por categorías. Respetan la idea que alimentación, hogar y ropa debe ir separado de eletrónica y juguetes. Además, las categorías quedan bien separadas según en qué cluster tienen el máximo de obersvaciones.

Así, el algoritmo K-medoides ha demostrado ofrecer mejores resultados a la hora de interpretarlos por categorías.

## Train - Test

Como hemos visto en las secciones anteriores, el algoritmo k-medoides ha demostrado ofrecer mejores resultados. Para validar este comportamiento, realizaremos una simulación dividiendo los datos en dos partes: utilizaremos el 80% de los datos para entrenar el modelo de k-medoides y el 20% restante se empleará como conjunto de prueba.

En esta etapa de evaluación, compararemos las predicciones generadas por el modelo con la clasificación original de los datos, lo que nos permitirá medir su precisión y rendimiento.

Separamos el conjunto de datos:

```{r}
set.seed(124) 
data <- raw_data_numeric

train_index <- createDataPartition(1:nrow(data), p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
test_labels <- pam_cluster$clustering[-train_index]
```

Entrenamos el modelo:

```{r}
k <- 3
kmedoids_model <- pam(train_data, k)
print(kmedoids_model)
```

Aplicamos el modelo al conjunto de prueba:

```{r}
medoids <- kmedoids_model$medoids
dist_test <- apply(test_data, 1, function(x) {
  apply(medoids, 1, function(y) sum(abs(x - y)))  # Distancia Manhattan
})

test_clusters <- apply(dist_test, 2, which.min)
```

Comparamos los resultados del modelo con los reales:

```{r}
cluster_factor <- factor(test_clusters, levels = 1:k)
sum(cluster_factor == test_labels)
```

Concluimos que para esta simulación el modelo ha acertado 35 veces de 40.


## Recomendaciones y conclusiones

Notemos que podemos separar las estrategias de marketing según electrónica (tercer grupo) y juguetes (primer grupo) mientras que para hogar, alimentacion y ropa se pueden diseñar estrategias de marketing parecidas (segundo grupo). Siguiendo la clasificación del k-medoides. 

- Compras de más artículos pero bajo gasto (*cluster juguetes*). 
    - Estrategias: Promociones que aumenten el numero de articulos vendidios, por ejemplo, 2x1 o 3x2.
  
- Compras de menor número de productos y gasto medio (*cluster alimentos, ropa y hogar*). 
    - Estrategias: como este es el sector en el que menos número de artículos se consume, proponemos hacer campañas para fomentar la compra de más artículos. 
  
- Compras de alto gasto y también muchos artículos (*cluster electrónica*).
    - Estrategias: Este sector está funcionando bien y se le está sacando provecho. La venta de electrónica no necesita tanto esfuerzo para mejorar los resultados comerciales.

Además, para futuros estudios proponemos estudiar los datos según la variable de perfil "fecha de la última compra". Sería interesante encontrar patrones de estacionalidad para enfocar la estrategia de marketing según la época del año.




## Bibliografia

-   [Ciencia de Datos](https://cienciadedatos.net/documentos/37_clustering_y_heatmaps#Hierarchical_clustering)
-   [Aprender R - UIB](https://aprender-uib.github.io/AD/)
-   [geeksforgeeks](https://www.geeksforgeeks.org/computing-classification-evaluation-metrics-in-r/)
